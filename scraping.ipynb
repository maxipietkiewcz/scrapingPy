{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawling con Python\n",
    "\n",
    "\n",
    "El objetivo de este trabajo práctico es implementar un web crawler en Python que pueda\n",
    "recorrer un sitio web, extraer todas las etiquetas <a> con sus respectivos enlaces y acceder a\n",
    "cada página enlazada. Por cada enlace encontrado, se deben obtener todas las etiquetas <h1> y\n",
    "<p> y almacenarlo en un array en un archivo JSON y si no se encuentran dichos elementos\n",
    "guardar el array como vacío. Por ejemplo, si el crawler encuentra un enlace\n",
    "https://example.com/pagina1 en una página y accede a ese enlace, debe obtener el contenido\n",
    "de la página https://example.com/pagina1 y buscar todos los elementos <h1></h1> y\n",
    "<p></p>, luego almacenarlo en un array en un archivo JSON bajo la clave\n",
    "\"https://example.com/pagina1\" con el array de los elementos solicitados de la página como\n",
    "valor correspondiente y si no se encuentran dichos elementos guardar el array vacío.\n",
    "\n",
    "\n",
    "1. Importamos las librerias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Creamos una funcion para extraer los datos de una pagina web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_website(url): \n",
    "    try:\n",
    "        response = requests.get(url)  # Realiza una solicitud GET a la URL proporcionada\n",
    "        response.raise_for_status()  # Verifica si hay errores HTTP en la respuesta\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')  # Crea un objeto BeautifulSoup para analizar el contenido HTML\n",
    "        \n",
    "        links = soup.find_all('a', href=True)[:100]  # Encuentra todas las etiquetas <a> con atributo href y obtiene los primeros 100 enlaces\n",
    "                                                     # Utilice [:100] para limitar la búsqueda a los primeros 100 enlaces para evitar sobrecarga de recursos\n",
    "                                                     # Para obtener todos los enlaces, elimine [:100] o comentelo\n",
    "\n",
    "        data = {}  # Inicializa un diccionario para almacenar los datos extraídos de las páginas enlazadas\n",
    "        \n",
    "        for link in links:  # Itera sobre cada enlace encontrado\n",
    "            href = link['href']  # Obtiene la URL del enlace\n",
    "            if href.startswith('http'):  # Verifica si el enlace comienza con 'http' (enlace externo)\n",
    "                try:\n",
    "                    page_response = requests.get(href)  # Realiza una solicitud GET al enlace\n",
    "                    page_response.raise_for_status()  # Verifica si hay errores HTTP en la respuesta\n",
    "                    page_soup = BeautifulSoup(page_response.text, 'html.parser')  # Crea un objeto BeautifulSoup para analizar el contenido HTML de la página enlazada\n",
    "                    h1_tags = page_soup.find_all('h1')  # Encuentra todas las etiquetas <h1> en la página enlazada\n",
    "                    p_tags = page_soup.find_all('p')  # Encuentra todas las etiquetas <p> en la página enlazada\n",
    "                    if h1_tags or p_tags:  # Verifica si hay etiquetas <h1> o <p> en la página enlazada\n",
    "                        data[href] = [str(tag) for tag in h1_tags + p_tags]  # Almacena las etiquetas encontradas como una lista de cadenas en el diccionario 'data'\n",
    "                    else:\n",
    "                        data[href] = []  # Si no se encuentran etiquetas <h1> ni <p>, almacena una lista vacía en el diccionario 'data'\n",
    "                except requests.exceptions.RequestException as e:  # Maneja las excepciones relacionadas con las solicitudes HTTP\n",
    "                    print(f\"Error al acceder a {href}: {e}\")  # Imprime un mensaje de error\n",
    "                except Exception as e:  # Maneja cualquier otro tipo de excepción\n",
    "                    print(f\"Error inesperado al procesar {href}: {e}\")  # Imprime un mensaje de error\n",
    "                    \n",
    "        return data  # Retorna el diccionario 'data' con los datos extraídos de las páginas enlazadas\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:  # Maneja las excepciones relacionadas con las solicitudes HTTP\n",
    "        print(\"Error al realizar la solicitud:\", e)  # Imprime un mensaje de error\n",
    "        return {}  # Retorna un diccionario vacío si ocurre un error en la solicitud HTTP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
