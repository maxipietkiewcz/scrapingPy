{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawling con Python\n",
    "\n",
    "El objetivo de este trabajo práctico es implementar un web crawler en Python que pueda\n",
    "recorrer un sitio web, extraer todas las etiquetas `<a>` con sus respectivos enlaces y acceder a\n",
    "cada página enlazada. Por cada enlace encontrado, se deben obtener todas las etiquetas `<h1>` y\n",
    "`<p>` y almacenarlo en un array en un archivo JSON y si no se encuentran dichos elementos\n",
    "guardar el array como vacío. Por ejemplo, si el crawler encuentra un enlace\n",
    "https://example.com/pagina1 en una página y accede a ese enlace, debe obtener el contenido\n",
    "de la página https://example.com/pagina1 y buscar todos los elementos `<h1></h1>` y\n",
    "`<p></p>`, luego almacenarlo en un array en un archivo JSON bajo la clave\n",
    "\"https://example.com/pagina1\" con el array de los elementos solicitados de la página como\n",
    "valor correspondiente y si no se encuentran dichos elementos guardar el array vacío.\n",
    "\n",
    "\n",
    "\n",
    "1. Importamos las librerias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Creamos una funcion para extraer los datos de una pagina web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rastrear_sitio_web(url): \n",
    "    try:\n",
    "        respuesta = requests.get(url) # Realiza una solicitud GET a la URL proporcionada\n",
    "        respuesta.raise_for_status()  # Verifica si hay errores HTTP en la respuesta\n",
    "        soup = BeautifulSoup(respuesta.text, 'html.parser')  # Crea un objeto BeautifulSoup para analizar el contenido HTML\n",
    "        \n",
    "        enlace = soup.find_all('a', href=True)[:100]  # Encuentra todas las etiquetas <a> con atributo href y obtiene los primeros 100 enlaces\n",
    "                                                      # Utilice [:100] para limitar la búsqueda a los primeros 100 enlaces para evitar sobrecarga de recursos\n",
    "                                                      # Para obtener todos los enlaces, elimine [:100] o comentelo\n",
    "\n",
    "        datos = {}  # Inicializa un diccionario para almacenar los datos extraídos de las páginas enlazadas\n",
    "        \n",
    "        for enlace in enlace:  # Itera sobre cada enlace encontrado\n",
    "            href = enlace['href']  # Obtiene la URL del enlace\n",
    "            if href.startswith('http'):  # Verifica si el enlace comienza con 'http', es decir, si es un enlace externo.\n",
    "                try:\n",
    "                    respuesta_pagina = requests.get(href)  # Realiza una solicitud GET al enlace\n",
    "                    respuesta_pagina.raise_for_status()  # Verifica si hay errores HTTP en la respuesta\n",
    "                    pagina_soup = BeautifulSoup(respuesta_pagina.text, 'html.parser')  # Crea un objeto BeautifulSoup para analizar el contenido HTML de la página enlazada\n",
    "                    etiquetas_h1 = pagina_soup.find_all('h1')  # Encuentra todas las etiquetas <h1> en la página enlazada\n",
    "                    etiquetas_p = pagina_soup.find_all('p')  # Encuentra todas las etiquetas <p> en la página enlazada\n",
    "                    if etiquetas_h1 or etiquetas_p:  # Verifica si hay etiquetas <h1> o <p> en la página enlazada\n",
    "                        datos[href] = [str(tag) for tag in etiquetas_h1 + etiquetas_p]  # Almacena las etiquetas encontradas como una lista de cadenas en el diccionario 'datos'\n",
    "                    else:\n",
    "                        datos[href] = []  # Si no se encuentran etiquetas <h1> ni <p>, almacena una lista vacía en el diccionario 'datos'\n",
    "                except requests.exceptions.RequestException as e:  # Maneja las excepciones relacionadas con las solicitudes HTTP\n",
    "                    print(f\"Error al acceder a {href}: {e}\") \n",
    "                except Exception as e:  # Maneja cualquier otro tipo de excepción\n",
    "                    print(f\"Error inesperado al procesar {href}: {e}\") \n",
    "                    \n",
    "        return datos  # Retorna el diccionario 'datos' con los datos extraídos de las páginas enlazadas\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:  # Maneja las excepciones relacionadas con las solicitudes HTTP\n",
    "        print(\"Error al realizar la solicitud:\", e)  # Imprime un mensaje de error\n",
    "        return {}  # Retorna un diccionario vacío si ocurre un error en la solicitud HTTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Creamos otra funcion en donde guardamos los datos extraidos en un archivo JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función que toma dos argumentos: `datos` (los datos a guardar) y `nombre_archivo` (el nombre del archivo).\n",
    "def guardar_en_json(datos, nombre_archivo):\n",
    "    \n",
    "    with open(nombre_archivo, 'w') as archivo:\n",
    "        # Abre el archivo especificado por `nombre_archivo` en modo escritura ('w').\n",
    "        # Utilizamos `with` para garantizar que el archivo se cierre correctamente. \n",
    "        \n",
    "        json.dump(datos, archivo, indent=4)\n",
    "        # Convierte los datos proporcionados (`datos`) a formato JSON y los escribe en el archivo.\n",
    "        # `indent=4` agrega una indentación de 4 espacios para hacer el archivo más legible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Luego verificamos si es el principal, definimos una URL y un archivo de salida, rastreamos el sitio web y guardamos los datos en un archivo JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Verifica si este script está siendo ejecutado como el programa principal.\n",
    "    \n",
    "    url = 'https://es.wikipedia.org/wiki/Wikipedia:Portada'\n",
    "    # Define la URL del sitio web que se va a rastrear.\n",
    "    \n",
    "    nombre_archivo_salida = 'datos_sitio_web.json'\n",
    "    # Define el nombre del archivo donde se guardarán los datos del sitio web.\n",
    "    \n",
    "    datos_sitio_web = rastrear_sitio_web(url)\n",
    "    # Llama a la función `rastrear_sitio_web` con la URL especificada para obtener los datos del sitio web.\n",
    "    \n",
    "    guardar_en_json(datos_sitio_web, nombre_archivo_salida)\n",
    "    # Guarda los datos del sitio web en un archivo JSON utilizando la función `guardar_en_json`.\n",
    "    # `datos_sitio_web` contiene los datos del sitio web obtenidos de `rastrear_sitio_web`.\n",
    "    # `nombre_archivo_salida` es el nombre del archivo donde se guardarán los datos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
